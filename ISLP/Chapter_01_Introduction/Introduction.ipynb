{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfPIX9FxCpvJiLbmy6d1ww"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div align=left style=\"margin-left:10%; margin-top:5%\">\n","<font face=\"LMRoman17-Regular\" size=10 color=\"0068b4\">\n","1<br>Introduction\n","</font>\n","</div>\n","\n","---\n"],"metadata":{"id":"ct8PL2hWysi9"}},{"cell_type":"markdown","source":["# An Overview of Statistical Learning\n","Statistical learning tools can be classified as **supervised** or **unsupervised**.\n","In **Supervised** learning we have an output based on one or more labeled inputs, While with **unsupervised** statistical learning, there are unlabeled inputs and\n","no supervising output.\n","Three real-world data sets that are considered in this book:\n","### Wage Data\n","It's referred to as <font color='#be5100'>Wage</font> data set throughout this book.\n","Number of factors that relate to wages for a group of men from the Atlantic region of the United States. In particular, we wish to understand the association between an employee’s <font color='#be5100'>age</font> and <font color='#be5100'>education</font>, as well as the calendar <font color='#be5100'>year</font>, on his <font color='#be5100'>wage</font>.\n","### Stock Market Data\n","In certain cases we may wish to predict a non-numerical value—that is, a categorical or qualitative output. For example, in Chapter 4 we examine a stock market data set that contains the daily movements in the Standard & Poor’s 500\n","(S&P) stock index over a 5-year period between 2001 and 2005. We refer to this as the <font color='#be5100'>Smarket</font> data.\n","The goal is to predict whether the index will increase or decrease on a given day, using the past 5 days’ percentage changes in the index.\n","### Gene Expression Data\n","Another important class of problems involves situations in which we only observe input variables, with no corresponding output. For example, in a marketing setting, we might have demographic information for a number of current or potential customers. We may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics. This is known as a **clustering** problem. Unlike in the previous examples, here we are not trying to predict an output variable.\n","\n","We devote Chapter 12 to a discussion of statistical learning methods for problems in which no natural output variable is available. We consider the <font color='#be5100'>NCI60</font> data set, which consists of 6,830 gene expression measurements for each of 64 cancer cell lines. Instead of predicting a particular output variable, we are interested in determining whether there are groups, or clusters, among the cell lines based on their gene expression measurements. This is a difficult question to address, in part because there are thousands of gene expression measurements per cell line, making it hard to visualize the data."],"metadata":{"id":"N91NX9XV_9wS"}},{"cell_type":"markdown","source":["\n","\n","# A Brief History of Statistical Learning\n","\n","1. At the beginning of the **nineteenth** century, the method of *least squares* was developed, implementing the earliest form of what is now known as *linear regression*. The approach was first successfully applied to problems in astronomy.\n","\n"," Linear regression is used for predicting **quantitative** values, such as an individual’s salary.\n","\n","2. In **1936**, to predict **qualitative** values, such as whether a patient survives or dies, or whether the stock market increases or decreases, *linear discriminant* analysis was proposed.\n","\n","3. In the **1940s**, various authors put forth an alternative approach: *logistic regression*.\n","\n","4. In the early **1970s**, the term *generalized linear model* was developed to describe an entire class of statistical learning methods that include both linear and logistic regression.\n","\n","5. By the **1980s**, computing technology had improved sufficiently that *non-linear* methods were no longer computationally prohibitive. In the mid **1980s**, *classification* and *regression trees* were developed,followed shortly by *generalized additive models*.\n","\n","6. *Neural networks* gained popularity in the **1980s**, and support *vector machines* arose in the **1990s**.\n","\n","7. Since that time, statistical learning has emerged as a new subfield in statistics, focused on supervised and unsupervised modeling and prediction. In recent years, progress in statistical learning has been marked by the\n"],"metadata":{"id":"o-Hn1ILBi4ay"}},{"cell_type":"markdown","source":["# Notation and Simple Matrix Algebra\n","\n","* $n$ : to represent the number of distinct data points, or observations, in our sample.\n","\n","* $p$ : denotes the number of variables that are available for use in making predictions.\n","\n","* $x_{ij}$ : represent the value of the $j$th variable for the\n","$i$th observation, where $i = 1, 2, ..., n$ and $j = 1, 2, ..., p$.\n","\n","* $\\mathbf{X}$ : denote an $n × p$ matrix whose $(i, j)$th element is $x_{ij}$:\n","\n","$$\n","\\mathbf{X} = \\begin{pmatrix}\n","x_{11} & x_{12} & ... & x_{1p} \\\\\n","x_{21} & x_{22} & ... & x_{2p} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","x_{n1} & x_{n2} & ... & x_{np} \\\\\n","\\end{pmatrix}\n","$$\n","<br>\n","\n","* $x_i$: the rows of $\\mathbf{X}$, which we write as $x_1, x_2, . . . ,x_n$. Here $x_i$ is a vector of length $p$, containing the $p$ variable measurements for the $i$th observation. That is,\n","\n","$$\n","x_i = \\begin{pmatrix}\n","x_{i1} \\\\\n","x_{i2} \\\\\n","\\vdots \\\\\n","x_{ip} \\\\\n","\\end{pmatrix}\n","$$\n","<br>\n","\n","* $x_j$: the columns of $\\mathbf{X}$, which we write as $\\mathbf{x}_1, \\mathbf{x}_2, . . . ,\\mathbf{x}_p$. Here $\\mathbf{x}_j$ is a vector of length $n$, containing the $n$ variable measurements for the $j$th observation. That is,\n","\n","$$\n","\\mathbf{x}_j = \\begin{pmatrix}\n","x_{1j} \\\\\n","x_{2j} \\\\\n","\\vdots \\\\\n","x_{nj} \\\\\n","\\end{pmatrix}\n","$$\n","\n","* $\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, . . . ,\\mathbf{x}_p)$ , or\n","\n","  $\\mathbf{X} = \\begin{pmatrix}\n","  x_1^T \\\\\n","  x_2^T \\\\\n","  \\vdots \\\\\n","  x_n^T \\\\\n","  \\end{pmatrix}$\n","<br>\n","\n","* The $^T$ notation denotes the transpose of a matrix or vector. So, for example,\n","$$\n","\\mathbf{X}^T = \\begin{pmatrix}\n","x_{11} & x_{21} & ... & x_{n1} \\\\\n","x_{12} & x_{22} & ... & x_{n2} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","x_{1p} & x_{2p} & ... & x_{np} \\\\\n","\\end{pmatrix}\n","$$\n","<br>\n","\n","* In this text, a vector of length \"$n$\" will always be denoted in lower case bold; e.g.\n","$$\n","\\mathbf{a} = \\begin{pmatrix}\n","a_1 \\\\\n","a_2 \\\\\n","\\vdots \\\\\n","a_n \\\\\n","\\end{pmatrix}\n","$$\n","However, vectors that are not of length $n$ (such as feature vectors of length $p$) will be denoted in lowercase normal font !\n","<br>\n","\n","* Matrix multiplication: Suppose that $A \\in \\mathbb{R} ^ {r×d}$ and $B \\in \\mathbb{R}^{d×s}$. Then $(AB)_{ij} = \\sum_{k=1}^d a_{ik}b_{kj}$"],"metadata":{"id":"4aQPqW_cUay1"}},{"cell_type":"markdown","source":["# Organization of This Book\n","1. [Chapter 1](https://colab.research.google.com/drive/1rVZoTW_6vTt1QZZA5Ngx-Ba8c-9qHD6C?usp=sharing) : Introduction\n","2. [Chapter 2](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : Introduces the basic terminology and concepts behind statistical learning and presents the K-nearest neighbor classifier\n","3. [Chapter 3](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : linear regression\n","4. [Chapter 4](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : logistic regression and linear discriminant analysis\n","5. [Chapter 5](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : cross-validation and the bootstrap\n","6. [Chapter 6](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : stepwise selection, ridge regression, principal components regression, and the lasso\n","7. [Chapter 7](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : introduce non-linear methods\n","8. [Chapter 8](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : tree-based methods, including bagging, boosting, and random forests\n","9. [Chapter 9](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : Support vector machines\n","10. [Chapter 10](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : deep learning\n","11. [Chapter 11](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : survival analysis\n","12. [Chapter 12](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : principal components analysis, K-means clustering, and hierarchical clustering\n","13. [Chapter 12](https://colab.research.google.com/drive/1I86AG8APgP4vls-HFjCVTDgJykFpnIsQ?usp=sharing) : multiple hypothesis testing.\n","\n","At the end of each chapter, we present one or more Python lab sections.\n","\n","the![picture](https://drive.google.com/uc?export=view&id=1DhoHFe5Uzd6VLYksRGAssvoc7cqq8qb-) denotes sections or exercises that contain more challenging concepts. These can be easily skipped by readers who do not wish to delve as deeply into the material, or who lack the mathematical background.\n"],"metadata":{"id":"FG5jZ3K_7o4e"}},{"cell_type":"markdown","source":["# Data Sets Used in Labs and Exercises\n","\n","Here we illustrate statistical learning methods using applications from marketing, finance, biology, and other areas.\n","The following table contains a summary of the data sets required to perform the labs and exercises.\n","\n","| Name | Description |\n","| :---: | :---: |\n","| <font color='#be5100'>Auto</font> | Gas mileage, horsepower, and other information for cars. |\n","| <font color='#be5100'>Bikeshare</font> | Hourly usage of a bike sharing program in Washington, DC. |\n","| <font color='#be5100'>Boston</font> | Housing values and other information about Boston census tracts. |\n","| <font color='#be5100'>BrainCancer</font> | Survival times for patients diagnosed with brain cancer. |\n","| <font color='#be5100'>Caravan</font> | Information about individuals offered caravan insurance. |\n","| <font color='#be5100'>Carseats</font> | Information about car seat sales in 400 stores. |\n","| <font color='#be5100'>College</font> | Demographic characteristics, tuition, and more for USA colleges. |\n","| <font color='#be5100'>Credit</font> | Information about credit card debt for 400 customers. |\n","| <font color='#be5100'>Default</font> | Customer default records for a credit card company. |\n","| <font color='#be5100'>Fund</font> | Returns of 2,000 hedge fund managers over 50 months. |\n","| <font color='#be5100'>Hitters</font> | Records and salaries for baseball players. |\n","| <font color='#be5100'>Khan</font> |Gene expression measurements for four cancer types. |\n","| <font color='#be5100'>NCI60</font> | Gene expression measurements for 64 cancer cell lines. |\n","| <font color='#be5100'>NYSE</font> | Returns, volatility, and volume for the New York Stock Exchange. |\n","| <font color='#be5100'>OJ</font> | Sales information for Citrus Hill and Minute Maid orange juice. |\n","| <font color='#be5100'>Portfolio</font> | Past values of financial assets, for use in portfolio allocation. |\n","| <font color='#be5100'>Publication</font> | Time to publication for 244 clinical trials. |\n","| <font color='#be5100'>Smarket</font> | Daily percentage returns for S&P 500 over a 5-year period. |\n","| <font color='#be5100'>USArrests</font> | Crime statistics per 100,000 residents in 50 states of USA. |\n","| <font color='#be5100'>Wage</font> | Income survey data for men in central Atlantic region of USA. |\n","| <font color='#be5100'>Weekly</font> |1,089 weekly stock market returns for 21 years. |\n","|  |  |\n","| |A list of data sets needed to perform the labs and exercises in this textbook.<br> All data sets are available in the <font color='#be5100'>ISLP</font> package, with the exception of <font color='#be5100'>USArrests</font>,<br> which is part of the base <font color='#be5100'>R</font> distribution, but accessible from <font color='#be5100'>Python</font>.|\n"],"metadata":{"id":"JTbqdiC24ekL"}},{"cell_type":"markdown","source":[],"metadata":{"id":"8fr7qUennvp4"}}]}