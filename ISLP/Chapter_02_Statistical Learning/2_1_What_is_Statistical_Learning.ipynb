{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"LMRoman17-Regular\" size=10 color=\"0068b4\">\n",
        "2<br>Statistical Learning\n",
        "</font>\n",
        "</div>\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "MEVpzH_Ft4GF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"LMRoman17-Regular\" size=6 color=\"0068b4\">\n",
        "2.1 What Is Statistical Learning?\n",
        "</font>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "We begin with some simple examples."
      ],
      "metadata": {
        "id": "n6xtEv1hi4Be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that we want to investigate the association between advertising and sales of a particular product. The <font color='#be5100'>Advertising</font> data set consists of the <font color='#be5100'>sales</font> of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: <font color='#be5100'>TV</font>, <font color='#be5100'>radio</font>, and <font color='#be5100'>newspaper</font>.\n",
        "<br><br>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Lj0qxysxwnOrqO3W7rAaJdznTtW4u6LF\" width=\"80%\"/>\n",
        "</div>\n",
        "<br><br>\n",
        "\n",
        "If we determine that there is an association between advertising and sales, then we can adjust advertising budgets, thereby indirectly increasing sales.\n",
        "\n",
        "## Input and Output Variables:\n",
        "In the example mentioned, the advertising budgets are **input variables** while\n",
        "<font color='#be5100'>sales</font> is an **output variable**. The input variables are typically denoted using the symbol $X$, with a subscript to distinguish them.So $X_1$ might be the <font color='#be5100'>TV</font> budget, $X_2$ the <font color='#be5100'>radio</font> budget, and $X_3$ the <font color='#be5100'>newspaper</font> budget.\n",
        "\n",
        "The inputs go by different names, such as **predictors**, **independent variables**, **features**, or sometimes just **variables**.\n",
        "\n",
        "The output variable—in this case, <font color='#be5100'>sales</font>—is often called the **response** or **dependent variable**, and is typically denoted using the symbol $Y$. Throughout this book, we will use all of these terms  interchangeably\n",
        "\n",
        "More generally, suppose that we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, ..., X_p$. We assume that there is some relationship between $Y$ and $X = (X_1, X_2, ..., X_p)$, which can be written in the very general form\n",
        "\n",
        "$$Y = f(X) + \\epsilon$$\n",
        "\n",
        "Here $f$ is some fixed but unknown function of $X_1, ..., X_p$, and $ϵ$ is a random error term, which is independent of $X$ and has mean zero. In this formulation, f represents the systematic information that X provides about $Y$."
      ],
      "metadata": {
        "id": "oOb5E9826dRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As another example, consider the left-hand panel of Figure below, a plot of <font color='#be5100'>income</font> versus <font color='#be5100'>years of education</font> for 30 individuals in the <font color='#be5100'>Income</font> data set.\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1rITFzpUCh9aMiBctBiIoPwiPqtAAU8kk\" width=\"70%\"/>\n",
        "</div>\n",
        "<br><br>\n",
        "\n",
        "The function $f$ that connects the input variable to the output variable is in general unknown. In this situation, one must estimate $f$ based on the observed points. Since <font color='#be5100'>Income</font> is a simulated data set, $f$ is known and is shown by the blue curve in the right-hand panel of the figure.\n",
        "The vertical lines represent the error terms $ϵ$. The errors have approximately mean zero."
      ],
      "metadata": {
        "id": "VRH2XLbzECZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, the function $f$ may involve more than one input variable. In the figure below, we plot <font color='#be5100'>income</font> as a function of <font color='#be5100'>years of education</font> and <font color='#be5100'>seniority</font>.\n",
        "\n",
        "<br>\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1z4Qb4ufR0ofz3X7VJF62AAFWT_LSDIZG\" width=\"50%\"/>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "The blue surface represents the true underlying relationship between <font color='#be5100'>income</font> and <font color='#be5100'>years of education</font> and <font color='#be5100'>seniority</font>, which is known since the data are simulated. The red dots indicate the observed values of these quantities for 30 individuals.\n",
        "\n",
        "=> In essence, statistical learning refers to a set of approaches for estimating $f$.\n"
      ],
      "metadata": {
        "id": "SDLJoPSrEzgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Estimate $f$ ?\n",
        "\n",
        "Two main reasons; **Prediction** and **Inference**\n",
        "\n",
        "## Prediction\n",
        "In many situations, a set of inputs $X$ are readily available, but the output $Y$ cannot be easily obtained. In this setting, since the error term averages to zero, we can predict $Y$ using\n",
        "$$\\hat{Y}=\\hat{f}(X)$$\n",
        "\n",
        "where $\\hat{f}$ represents our estimate for $f$, and $\\hat{Y}$ for $Y$.\n",
        "In this setting, $\\hat{f}$ is often treated as a black box and typically we are not concerned with the exact form of $\\hat{f}$, provided that it yields accurate predictions for $Y$.\n",
        "\n",
        "The accuracy of $\\hat{Y}$ as a prediction for $Y$ depends on two quantities, which we will call the \"**reducible error**\" and the \"**irreducible error**\".\n",
        "In general, reducible error irreducible error $\\hat{f}$ will not be a perfect estimate for $f$, and this inaccuracy will introduce some error. This error is **reducible** because we can potentially improve the accuracy of $\\hat{f}$ by using an appropriate technique.\n",
        "\n",
        "However, even if it were possible to form a perfect estimate for $f$, so that our estimated response took the form $\\hat{Y}$ = $f(X)$, our prediction would still have some error in it! This is because $Y$ is also a function of $\\epsilon$, which by definition, $\\epsilon$ is independent from $X$ and cannot be predicted using that. Therefore, variability associated with $\\epsilon$ also affects the accuracy of our predictions. This is known as the **irreducible** error.\n",
        "\n",
        "Why is the irreducible error larger than zero? The quantity $\\epsilon$ may contain unmeasured variables that are useful in predicting $Y$: since we don’t measure them, f cannot use them for its prediction. The quantity $\\epsilon$ may also contain unmeasurable variation.\n",
        "\n",
        "Consider a given estimate $\\hat{f}$ and a set of predictors $X$, which yields the prediction $\\hat{Y}$ = $\\hat{f}(X)$. Assume for a moment that both $\\hat{f}$ and $X$ are fixed, so that the only variability comes from $\\epsilon$. Then, it is easy to show that\n",
        "\n",
        "$$\\begin{align}E[(Y-\\hat{Y})^2] &= E[(f(X)+\\epsilon-\\hat{f}(X))^2]\\\\\n",
        "                                &= [f(X)-\\hat{f}(X)]^2 + Var(\\epsilon) \\quad ,\n",
        "\\end{align}$$\n",
        "\n",
        "Where $E$ represents the average, or \"expected value\" and $Var(\\epsilon)$ represents the variance value. Note that $E(\\epsilon) = 0$ !\n",
        "\n",
        "\n",
        "=> It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for $Y$. This bound is almost always unknown in practice.\n",
        "\n",
        "\n",
        "## Inference\n",
        "\n",
        "We are often interested in understanding the association between $Y$ and $X_1, ..., X_p$. In this situation, we wish to estimate $f$, but our goal is not necessarily to make predictions for $Y$. Now $\\hat{f}$ cannot be treated as a black box, because we need to know its exact form. Some questions may arise:\n",
        "\n",
        "* Which predictors are associated with the response?\n",
        "\n",
        "  It is often the case that only a small fraction of the available predictors are substantially associated with $Y$. Depending on the application, identifying the few important predictors among a large set of possible variables can be extremely useful.\n",
        "\n",
        "\n",
        "* What is the relationship between the response and each predictor?\n",
        "\n",
        "  Some predictors may have a positive relationship with $Y$, in the sense that larger values of the predictor are associated with larger values of $Y$. Other predictors may have the opposite relationship. Depending on the complexity of $f$, the relationship between the response and a given predictor may also depend on the values of the other predictors.\n",
        "\n",
        "* Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n",
        "\n",
        "  Historically, most methods for estimating $f$ have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.\n",
        "\n",
        "In this book, we will see a number of examples that fall into the prediction setting, the inference setting, or a combination of the two."
      ],
      "metadata": {
        "id": "_8O3ggxYJCZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Do We Estimate $f$?\n",
        "\n",
        "We will always assume that we have observed a set of $n$ different data points. These observations are called the \"**training data**\" because we will use these training observations to train, or teach, our method how to estimate $f$.\n",
        "\n",
        "Most statistical learning methods for this task can be characterized as either parametric or non-parametric.\n",
        "\n",
        "## Parametric Methods\n",
        "Parametric methods involve a two-step \"model-based\" approach.\n",
        "1. First, we make an assumption about the functional form, or shape, of $f$. For example, one very simple assumption is that $f$ is linear in $X$:\n",
        "\n",
        "  $$ f(X)=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_pX_p$$\n",
        "  \n",
        "  this is a \"**linear model**\", which will be discussed extensively in Chapter 3. Once we have assumed that $f$ is linear, the problem of estimating $f$ is greatly simplified. Instead of having to estimate an entirely arbitrary $p$-dimensional function $f(X)$, one only needs to estimate the $p + 1$ coefficients $\\beta_0, \\beta_1, ...,\\beta_p$.\n",
        "\n",
        "\n",
        "2. After a model has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model, we need to estimate the parameters $\\beta_0, \\beta_1, ...,\\beta_p$. That is, we want to find values of these parameters such that\n",
        "\n",
        "  $$ Y\\approx\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_pX_p$$\n",
        "\n",
        "  The most common approach to fitting the linear model is referred to as (ordinary) least squares, which we discuss in Chapter 3. However, least squares is one of many possible ways to fit the linear model. In Chapter 6, we discuss other approaches for estimating the parameters.\n",
        "\n",
        "The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of $f$. If the chosen model is too far from the true $f$, then our estimate will be poor. We can try to address this problem by choosing flexible models that can fit many different possible functional forms for $f$. But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as \"**overfitting**\" the data, which essentially means they follow the errors, or noise, too closely.\n",
        "\n",
        "## Non-Parametric Methods\n",
        "\n",
        "Non-parametric methods do not make explicit assumptions about the functional form of $f$. Instead, they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly. Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for $f$, they have the potential to accurately fit a wider range of possible shapes for $f$. Any parametric approach brings with it the possibility that the functional form used to estimate $f$ is very different from the true $f$, in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of $f$ is made. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$."
      ],
      "metadata": {
        "id": "dGfG1adRJaz5"
      }
    }
  ]
}